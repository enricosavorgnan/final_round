
#%%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.callbacks import History, EarlyStopping
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.metrics import mean_squared_error
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.layers import Dropout
from tensorflow.keras.regularizers import l2
from tensorflow.keras.optimizers import Adam
#%% md
<p>First of all, we need to adjust data in order to be trainable by the Neural Network</p>
#%%
data = pd.read_csv('datasets/dataset.csv')

# Convert time to minutes from midnight and extract month from date
# data['time'] = pd.to_datetime(data['time']).dt.hour * 60 + pd.to_datetime(data['time']).dt.minute
# data['date'] = pd.to_datetime(data['date']).dt.month
data['date'] = np.array(pd.to_datetime(data['date']).dt.month)[0]
data['time'] = np.array(pd.to_datetime(data['time'].tail(1), format='%H:%M:%S').dt.hour * 60 + pd.to_datetime(data['time'].tail(1), format='%H:%M:%S').dt.minute)[0]

features = data.drop(columns=['window1_tg', 'window2_tg', 'window3_tg', 'window4_tg', 'shutter1_tg', 'shutter2_tg', 'shutter3_tg', 'shutter4_tg'])
targets = data[['window1_tg', 'window2_tg', 'window3_tg', 'window4_tg', 'shutter1_tg', 'shutter2_tg', 'shutter3_tg', 'shutter4_tg']]

# Normalize numerical features
numerical_features = features.select_dtypes(include=[np.number]).columns.tolist()
scaler = StandardScaler()
features[numerical_features] = scaler.fit_transform(features[numerical_features])

# Encode categorical features
# encoder = OneHotEncoder()
# categorical_features = ['date']  # Add other categorical feature names if necessary
# features_encoded = encoder.fit_transform(data[categorical_features]).toarray()
# features_encoded_df = pd.DataFrame(features_encoded, columns=encoder.get_feature_names_out(categorical_features))
# features = pd.concat([features.drop(columns=categorical_features), features_encoded_df], axis=1)

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)
#%% md
<p>Now it's time to actually train the model:</p>
#%%
model = Sequential([
    Dense(64, input_shape=(X_train.shape[1], ), activation='relu'),
    Dense(32, activation='relu'),
    Dense(targets.shape[1], activation='linear')  # Adjust the output layer to match the number of target variables
])

# Compile the model
model.compile(optimizer='adam', loss='mse')

# Early stopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Train the model
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stopping])
#%%
# Plot training and validation loss
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss Progression')
plt.xlabel('Epoch')
plt.ylabel('Loss (log)')
plt.legend()
plt.show()

# Evaluate the model on the test set
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error on Test Set:", mse)
#%% md
<p>After training, I save the model in a file</p>
#%%
model.save('my_model.keras')
#%%
model.predict(X_test)
